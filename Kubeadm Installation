# Prerequisites
=> At least two Ubuntu 24.04 LTS servers with 2GB RAM and 2 CPU cores (AWS t2.medium).
=> Network connectivity between servers --- ping -- private ip ( ping ).
=> Root access to each server 

# Update the system and install dependencies:
  $sudo apt update && sudo apt upgrade -y
  $sudo apt install apt-transport-https curl -y

#Install containerd:
  $sudo apt install containerd -y  

# Configure containerd:
  $sudo mkdir -p /etc/containerd
  $sudo containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
  $sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml 
  $sudo systemctl restart containerd
  $sudo systemctl status containerd (verify containerd start or not)

# Disable swap:
  $sudo swapoff -a
  $sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# Load necessary kernel modules:
  $sudo modprobe overlay
  $sudo modprobe br_netfilter

# Set required sysctl parameters:
  $cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
  net.bridge.bridge-nf-call-iptables  = 1
  net.bridge.bridge-nf-call-ip6tables = 1
  net.ipv4.ip_forward                 = 1
  EOF
  Ctrl + d
  $sudo sysctl --system

# Install Kubernetes components:
  $curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  $echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
  $sudo apt update
  $sudo apt install -y kubelet kubeadm kubectl 
  $sudo apt-mark hold kubelet kubeadm kubectl
---------------------------------------------------- Till this we have to run all node that in cluster-----------------------------------------

After this we have to run on master node ( control-plan) 
# Initialize the cluster
  $sudo kubeadm init --pod-network-cidr=10.244.0.0/16  ( dont change this ip ) ---- this is for flannel netwrok plugin

# Set up kubeconfig for the user:
  $export KUBECONFIG=$HOME/.kube/config

# Install Flannel network plugin (run only on master node)
  $wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
  $kubectl apply -f kube-flannel.yml

# Verify the installation:
  $kubectl get nodes
  $kubectl get pods --all-namespaces

# Join worker nodes to the cluster:
  Use the kubeadm join command provided by the kubeadm init output on the master node as root user or with sudo.
  Before join open port 6443
  $sudo systemctl restart containerd
  $sudo systemctl restart kubelet

# Verify worker node is join run below command on control plan or Master node
  $kubectl get nodes 

ðŸ‘‰ If you dont copy kubeadm join command provided by the kubeadm init output on the master node You can regenerate the Kubernetes join command
  kubeadm token create --print-join-command
âš  If worker node was already joined earlier you can reset it & rejoin Cluster by join command.
  kubeadm reset -f 

----------------------------------------------------------------------------------------------------
ðŸ‘‰ Key Reasons for Opening Port 6443
Control Plane to API Server:--- Other control plane components, such as the kube-controller-manager and kube-scheduler, communicate with the API server over this port.
Worker Nodes to API Server:--- Worker nodes must be able to connect to the API server to receive instructions (e.g., which pods to run), report status, and access information.
Clients to API Server:--- Users and external clients using tools like kubectl connect to this port to interact with the cluster.

ðŸ‘‰ Port 10256 :- kube-proxy health check server. This endpoint, usually at 0.0.0.0:10256/healthz, allows load balancers or other components to verify that the kube-proxy service is running and properly managing network rules for Kubernetes Services, ensuring the node's network health. 

ðŸ‘‰ Port 10250 :- it is the default secure port used by the Kubelet. The Kubelet is the primary node agent that ensures containers are running in a pod and healthy. 

  



  




